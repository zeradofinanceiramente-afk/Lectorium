
import { GoogleGenAI, Type } from "@google/genai";
import { ChatMessage, MindMapData } from "../types";
import { getStoredApiKey } from "../utils/apiKeyUtils";

// --- CONFIG ---
const getAiClient = () => {
  // 1. Tenta a chave do usu√°rio primeiro (LocalStorage)
  const userKey = getStoredApiKey();
  if (userKey) {
    return new GoogleGenAI({ apiKey: userKey });
  }
  // 2. Fallback para a chave do ambiente (se existir)
  if (process.env.API_KEY) {
    return new GoogleGenAI({ apiKey: process.env.API_KEY });
  }
  throw new Error("Chave de API n√£o configurada. Por favor, adicione sua chave nas configura√ß√µes.");
};

// --- RAG UTILS (Local Search) ---

// Stopwords b√°sicas em Portugu√™s e Ingl√™s para melhorar a busca
const STOP_WORDS = new Set([
  'o', 'a', 'os', 'as', 'um', 'uma', 'de', 'do', 'da', 'em', 'no', 'na', 'para', 'com', 'por', 'que', 'e', '√©', 
  'the', 'a', 'an', 'of', 'in', 'on', 'for', 'with', 'by', 'that', 'and', 'is', 'to'
]);

// Divide o texto em blocos l√≥gicos (par√°grafos)
function chunkText(fullText: string, maxChunkSize = 1000): string[] {
  const cleanText = fullText.replace(/\r\n/g, '\n');
  let rawChunks = cleanText.split(/\n\s*\n/);
  const finalChunks: string[] = [];
  for (const chunk of rawChunks) {
    if (chunk.length > maxChunkSize) {
      const sentences = chunk.match(/[^.!?]+[.!?]+[\])'"]*/g) || [chunk];
      let currentChunk = "";
      for (const sentence of sentences) {
        if (currentChunk.length + sentence.length > maxChunkSize) {
          finalChunks.push(currentChunk.trim());
          currentChunk = sentence;
        } else {
          currentChunk += sentence;
        }
      }
      if (currentChunk) finalChunks.push(currentChunk.trim());
    } else if (chunk.trim().length > 30) {
      finalChunks.push(chunk.trim());
    }
  }
  return finalChunks;
}

function scoreChunk(chunk: string, queryTerms: string[]): number {
  const normalizedChunk = chunk.toLowerCase();
  let score = 0;
  const EXACT_MATCH_BONUS = 3;
  const PARTIAL_MATCH_BONUS = 1;
  for (const term of queryTerms) {
    if (normalizedChunk.includes(term)) {
      const regex = new RegExp(`\\b${term}\\b`, 'gi');
      const matches = normalizedChunk.match(regex);
      if (matches) {
        score += matches.length * EXACT_MATCH_BONUS;
      } else {
        score += PARTIAL_MATCH_BONUS;
      }
    }
  }
  return score;
}

function findRelevantChunks(documentText: string, query: string, topK = 4): string[] {
  if (!documentText) return [];
  const queryTerms = query.toLowerCase()
    .replace(/[^\w\s√†-√∫]/g, '')
    .split(/\s+/)
    .filter(w => w.length > 2 && !STOP_WORDS.has(w));
  if (queryTerms.length === 0) return [documentText.slice(0, 2000)];
  const chunks = chunkText(documentText);
  const scoredChunks = chunks.map(chunk => ({
    text: chunk,
    score: scoreChunk(chunk, queryTerms)
  }));
  scoredChunks.sort((a, b) => b.score - a.score);
  const hasMatches = scoredChunks.some(c => c.score > 0);
  const relevant = hasMatches ? scoredChunks.filter(c => c.score > 0) : scoredChunks;
  return relevant.slice(0, topK).map(c => c.text);
}

// --- AI FUNCTIONS ---

/**
 * Gera embeddings vetoriais para uma lista de textos usando o modelo text-embedding-004.
 * Retorna uma lista de vetores (Float32Array) correspondentes.
 */
export async function generateEmbeddings(texts: string[]): Promise<Float32Array[]> {
  const ai = getAiClient();
  const model = "text-embedding-004";
  
  // Limita o batch para evitar erros de limite da API
  // O modelo embedding suporta batch, mas vamos ser conservadores
  const embeddings: Float32Array[] = [];
  
  for (const text of texts) {
      try {
          const result = await ai.models.embedContent({
              model: model,
              content: { parts: [{ text }] }
          });
          
          if (result.embedding && result.embedding.values) {
              embeddings.push(new Float32Array(result.embedding.values));
          } else {
              // Fallback vetor zero ou skip? Melhor skip para n√£o sujar a busca.
              // Mas para manter √≠ndice alinhado, pushamos null ou zero.
              console.warn("Embedding vazio retornado para:", text.slice(0, 20));
              embeddings.push(new Float32Array(0)); 
          }
      } catch (e) {
          console.error("Erro ao gerar embedding:", e);
          embeddings.push(new Float32Array(0));
      }
  }
  return embeddings;
}

export async function extractNewspaperContent(base64Image: string, mimeType: string) {
  const ai = getAiClient();
  const prompt = `Voc√™ √© um arquivista digital. Analise esta p√°gina de jornal hist√≥rico.
  O documento foi pr√©-processado para destacar a estrutura visual.
  1. Identifique as not√≠cias seguindo a hierarquia de colunas (da esquerda para a direita).
  2. Extraia o t√≠tulo e o corpo de cada mat√©ria.
  3. Reconstrua par√°grafos que possam ter sido interrompidos por quebras de coluna.
  4. Identifique entidades (nomes, datas, locais) citadas.
  Retorne os dados em JSON estruturado.`;

  try {
    const response = await ai.models.generateContent({
      model: 'gemini-3-pro-preview',
      contents: {
        parts: [
          { inlineData: { data: base64Image, mimeType } },
          { text: prompt }
        ]
      },
      config: {
        responseMimeType: "application/json",
        responseSchema: {
          type: Type.OBJECT,
          properties: {
            articles: {
              type: Type.ARRAY,
              items: {
                type: Type.OBJECT,
                properties: {
                  title: { type: Type.STRING },
                  content: { type: Type.STRING },
                  columnSpan: { type: Type.STRING, description: "Ex: 'Coluna 1' ou 'Colunas 1-2'" },
                  sentiment: { type: Type.STRING },
                  summary: { type: Type.STRING }
                },
                required: ["title", "content"]
              }
            },
            publication: { type: Type.STRING },
            inferredDate: { type: Type.STRING }
          }
        }
      }
    });
    return JSON.parse(response.text || "{}");
  } catch (e) {
    console.error("Historical extraction failed", e);
    throw e;
  }
}

export async function refineOcrWords(words: string[]): Promise<string[]> {
  const ai = getAiClient();
  const prompt = `Abaixo est√° uma lista de palavras de um documento antigo extra√≠das via OCR.
  O fluxo de leitura foi preservado respeitando as colunas originais do layout.
  Corrija erros de reconhecimento tipogr√°fico (ex: 'f' lido como 's', '1' como 'l') mantendo o sentido acad√™mico.
  IMPORTANTE: Retorne exatamente o mesmo n√∫mero de itens.
  
  PALAVRAS:
  ${words.join(' ')}`;

  try {
    const response = await ai.models.generateContent({
      model: 'gemini-3-flash-preview',
      contents: prompt,
      config: {
        responseMimeType: "application/json",
        responseSchema: {
          type: Type.OBJECT,
          properties: {
            correctedWords: {
              type: Type.ARRAY,
              items: { type: Type.STRING }
            }
          },
          required: ["correctedWords"]
        }
      }
    });
    const result = JSON.parse(response.text || '{"correctedWords": []}');
    const corrected = result.correctedWords || [];
    if (corrected.length === words.length) return corrected;
    return words;
  } catch (e) {
    return words;
  }
}

export async function expandNodeWithAi(nodeText: string, context: string): Promise<string[]> {
  const ai = getAiClient();
  const prompt = `Sugira 3 conceitos para expandir "${nodeText}" no contexto de "${context}". Curto e direto. JSON array.`;
  try {
    const response = await ai.models.generateContent({
      model: 'gemini-3-flash-preview',
      contents: prompt,
      config: { responseMimeType: "application/json" }
    });
    return JSON.parse(response.text || "[]");
  } catch (e) {
    return [];
  }
}

export async function generateMindMapAi(topic: string): Promise<MindMapData> {
    const ai = getAiClient();
    const prompt = `Crie uma estrutura inicial de mapa mental para o assunto: "${topic}".
    Retorne um JSON seguindo exatamente esta interface:
    interface MindMapNode {
      id: string; text: string; x: number; y: number; width: number; height: number; color: string; parentId?: string; isRoot?: boolean; shape?: 'rectangle' | 'circle' | 'pill';
    }
    interface MindMapEdge { id: string; from: string; to: string; }
    interface MindMapData { nodes: MindMapNode[]; edges: MindMapEdge[]; viewport: {x: number, y: number, zoom: number}; }
    
    Regras:
    1. O n√≥ raiz (isRoot: true) deve estar em x:0, y:0.
    2. Crie de 4 a 7 sub-n√≥s distribu√≠dos ao redor.
    3. Use cores vibrantes acad√™micas.
    4. O JSON deve ser o √∫nico retorno.`;

    try {
        const response = await ai.models.generateContent({
            model: 'gemini-3-pro-preview',
            contents: prompt,
            config: { responseMimeType: "application/json" }
        });
        return JSON.parse(response.text || "{}");
    } catch (e) {
        console.error("AI MindMap generation failed", e);
        throw new Error("Falha ao gerar mapa com IA.");
    }
}

/**
 * Chat Stream with Local RAG Strategy
 */
export async function* chatWithDocumentStream(documentText: string, history: ChatMessage[], message: string) {
  const ai = getAiClient();
  
  // 1. RAG: Encontrar trechos relevantes para a pergunta ATUAL
  const relevantChunks = findRelevantChunks(documentText, message);
  const contextString = relevantChunks.length > 0 
    ? relevantChunks.join("\n\n---\n\n") 
    : "Documento vazio ou sem texto leg√≠vel dispon√≠vel no momento.";
  
  // 2. Mapear hist√≥rico do formato interno para o formato do Gemini SDK
  // Excluindo a √∫ltima mensagem do usu√°rio (que ser√° enviada via sendMessage)
  // IMPORTANTE: Mapeia roles 'model' para 'model' e 'user' para 'user'
  const previousHistory = history.slice(0, -1).map(msg => ({
    role: msg.role === 'model' ? 'model' : 'user',
    parts: [{ text: msg.text }],
  }));

  const systemInstruction = `Voc√™ √© o Lectorium AI, o n√∫cleo de intelig√™ncia anal√≠tica do Lectorium. Voc√™ n√£o √© apenas um chatbot; voc√™ √© um analista de sistemas s√™nior e assistente de pesquisa acad√™mica de alto desempenho.
Sua miss√£o: Processar conhecimento com precis√£o cir√∫rgica, mantendo a soberania dos dados do usu√°rio e a integridade das normas ABNT.

DIRETRIZES DE COMPORTAMENTO (O "ESTILO JARVIS"):
1. Anticonformismo e Cr√≠tica: N√£o tente agradar o usu√°rio. Seja direto, t√©cnico e, se necess√°rio, questione a premissa da pergunta se ela for mediocre. O usu√°rio valoriza a ousadia e o rigor intelectual.
2. Ousadia Did√°tica: N√£o apenas resuma. Conecte os pontos. Se o documento menciona "X" e a literatura acad√™mica externa sugere "Y", aponte a contradi√ß√£o.
3. Fontes H√≠bridas (RAG + Web):
   * Sua prioridade zero √© o CONTEXTO RELEVANTE fornecido pelo documento local.
   * Enriquecimento Externo: Voc√™ tem permiss√£o para usar seus conhecimentos de escritos acad√™micos consagrados para expandir a resposta, mas DEVE diferenciar o que √© do documento e o que √© conhecimento externo.
4. Cita√ß√£o Obrigat√≥ria: Use colchetes para cita√ß√µes [Autor, Ano] ou [P√°gina X]. Se a informa√ß√£o n√£o existir em lugar nenhum, seja honesto: "Informa√ß√£o ausente no documento e na base de conhecimento acad√™mica".
5. Transcri√ß√£o vs. S√≠ntese: Pedidos de "transcri√ß√£o" exigem fidelidade 1:1 (UTF-8 puro). Outros pedidos exigem s√≠ntese anal√≠tica de alta densidade.
6. Restri√ß√£o Est√©tica (Clean UI): √â terminantemente PROIBIDO o uso de Markdown de negrito (**) ou it√°lico (_). O Lectorium utiliza uma interface de alta performance baseada em texto plano para evitar ru√≠do visual. Use listas numeradas ou hifens para estrutura.

üìö CONTEXTO RELEVANTE (LOCAL-FIRST DATA):
${contextString}

üåê CONHECIMENTO ACAD√äMICO AMPLIADO:
Ao responder, integre conceitos de autores cl√°ssicos e contempor√¢neos relevantes ao tema acima, sempre citando-os para manter o padr√£o cient√≠fico.`;

  try {
    const chat = ai.chats.create({
      model: 'gemini-3-flash-preview',
      history: previousHistory,
      config: { systemInstruction, temperature: 0.2 }
    });
    
    const responseStream = await chat.sendMessageStream({ message });
    
    for await (const chunk of responseStream) {
      yield chunk.text || "";
    }
  } catch (e: any) {
    if (e.message.includes('API key')) {
        yield "Erro: Chave de API inv√°lida ou n√£o configurada. Configure no menu lateral.";
    } else {
        yield "Erro na conex√£o com a IA. Tente novamente.";
    }
  }
}
